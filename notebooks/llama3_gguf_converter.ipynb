{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "source": [
    "# Step 1: Install dependencies\n",
    "## This notebook converts a downloaded Llama-3 model (e.g., Llama-3-8B-Instruct) to GGUF format for use with llama.cpp.\n",
    "\n",
    "!pip install --quiet git+https://github.com/ggerganov/llama.cpp\n",
    "!pip install --quiet huggingface_hub transformers sentencepiece"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b95a48346fa98978"
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Authenticate HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "b95a48346fa98978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 3: Download model weights\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Change model_id if you want a different model (Llama-3-8B-Instruct, Llama-3-70B, etc.)\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# This will create a directory in /content\n",
    "weights_dir = snapshot_download(repo_id=model_id)\n",
    "print(f\"Model downloaded to: {weights_dir}\")\n"
   ],
   "id": "eb5315666981b342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List files in weights_dir to verify download\n",
    "import os\n",
    "from pathlib import Path\n",
    "import humanize\n",
    "\n",
    "print(f\"Files in {weights_dir}:\")\n",
    "for file in Path(weights_dir).rglob(\"*\"):\n",
    "    if file.is_file():\n",
    "        size = humanize.naturalsize(file.stat().st_size, gnu=True)\n",
    "        print(f\"{file.relative_to(weights_dir)}\\t{size}\")\n"
   ],
   "id": "6c75dd5886d631cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# If only small files are present, check your HuggingFace access token permissions and model license acceptance.",
   "id": "25d4ddeab461d5b4"
  },
  {
   "metadata": {
    "id": "537bcd62ae20467"
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Clone llama.cpp repository\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "%cd llama.cpp/scripts"
   ],
   "id": "537bcd62ae20467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "74f0a0e7818913c8"
   },
   "cell_type": "code",
   "source": [
    "# Step 5: Convert Llama-3 model to GGUF format\n",
    "# You can save the GGUF model file directly to Google Drive by setting the path below.\n",
    "# For example: gguf_out = '/content/drive/MyDrive/llama3-8b.gguf' (requires Drive to be mounted).\n",
    "\n",
    "import os\n",
    "\n",
    "gguf_out = \"/content/llama3-8b.gguf\"\n",
    "# If you want to save directly to Google Drive, set gguf_out = '/content/drive/MyDrive/llama3-8b.gguf' (requires Drive mounted).\n",
    "# Run conversion script (for Llama-3 only)\n",
    "%cd /content/llama.cpp/\n",
    "!python3 convert_hf_to_gguf.py \\\n",
    "  --outtype f16 \\\n",
    "  --outfile {gguf_out} \\\n",
    "  {weights_dir}\n",
    "\n",
    "print(f\"GGUF model saved to: {gguf_out}\")"
   ],
   "id": "74f0a0e7818913c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b9c8b6026d46c27b"
   },
   "cell_type": "code",
   "source": [
    "# Step 6: Download the GGUF model file\n",
    "from google.colab import files\n",
    "files.download(gguf_out)"
   ],
   "id": "b9c8b6026d46c27b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
