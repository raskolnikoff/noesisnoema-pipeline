{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"initial_id\",\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true,\n",
    "    \"id\": \"initial_id\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Step 1: Install dependencies\\n\",\n",
    "    \"## This notebook converts a downloaded Llama-3 model (e.g., Llama-3-8B-Instruct) to GGUF format for use with llama.cpp.\\n\",\n",
    "    \"\\n\",\n",
    "    \"!pip install --quiet git+https://github.com/ggerganov/llama.cpp\\n\",\n",
    "    \"!pip install --quiet huggingface_hub transformers sentencepiece\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"id\": \"b95a48346fa98978\"\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Step 2: Authenticate HuggingFace\\n\",\n",
    "    \"from huggingface_hub import notebook_login\\n\",\n",
    "    \"notebook_login()\"\n",
    "   ],\n",
    "   \"id\": \"b95a48346fa98978\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Step 3: Download model weights\\n\",\n",
    "    \"from huggingface_hub import snapshot_download\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Change model_id if you want a different model (Llama-3-8B-Instruct, Llama-3-70B, etc.)\\n\",\n",
    "    \"model_id = \\\"meta-llama/Meta-Llama-3-8B-Instruct\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# This will create a directory in /content\\n\",\n",
    "    \"weights_dir = snapshot_download(repo_id=model_id)\\n\",\n",
    "    \"print(f\\\"Model downloaded to: {weights_dir}\\\")\\n\"\n",
    "   ],\n",
    "   \"id\": \"eb5315666981b342\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# List files in weights_dir to verify download\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import humanize\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Files in {weights_dir}:\\\")\\n\",\n",
    "    \"for file in Path(weights_dir).rglob(\\\"*\\\"):\\n\",\n",
    "    \"    if file.is_file():\\n\",\n",
    "    \"        size = humanize.naturalsize(file.stat().st_size, gnu=True)\\n\",\n",
    "    \"        print(f\\\"{file.relative_to(weights_dir)}\\\\t{size}\\\")\\n\"\n",
    "   ],\n",
    "   \"id\": \"6c75dd5886d631cd\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": \"# If only small files are present, check your HuggingFace access token permissions and model license acceptance.\",\n",
    "   \"id\": \"25d4ddeab461d5b4\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"id\": \"537bcd62ae20467\"\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Step 4: Clone llama.cpp repository\\n\",\n",
    "    \"!git clone https://github.com/ggerganov/llama.cpp.git\\n\",\n",
    "    \"%cd llama.cpp/scripts\"\n",
    "   ],\n",
    "   \"id\": \"537bcd62ae20467\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"id\": \"74f0a0e7818913c8\"\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Step 5: Convert Llama-3 model to GGUF format\\n\",\n",
    "    \"# You can save the GGUF model file directly to Google Drive by setting the path below.\\n\",\n",
    "    \"# For example: gguf_out = '/content/drive/MyDrive/llama3-8b.gguf' (requires Drive to be mounted).\\n\",\n",
    "    \"\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"gguf_out = \\\"/content/llama3-8b.gguf\\\"\\n\",\n",
    "    \"# If you want to save directly to Google Drive, set gguf_out = '/content/drive/MyDrive/llama3-8b.gguf' (requires Drive mounted).\\n\",\n",
    "    \"# Run conversion script (for Llama-3 only)\\n\",\n",
    "    \"%cd /content/llama.cpp/\\n\",\n",
    "    \"!python3 convert_hf_to_gguf.py \\\\\\n\",\n",
    "    \"  --outtype f16 \\\\\\n\",\n",
    "    \"  --outfile {gguf_out} \\\\\\n\",\n",
    "    \"  {weights_dir}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"GGUF model saved to: {gguf_out}\\\")\"\n",
    "   ],\n",
    "   \"id\": \"74f0a0e7818913c8\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"id\": \"b9c8b6026d46c27b\"\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Step 6: Download the GGUF model file\\n\",\n",
    "    \"from google.colab import files\\n\",\n",
    "    \"files.download(gguf_out)\"\n",
    "   ],\n",
    "   \"id\": \"b9c8b6026d46c27b\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  },\n",
    "  \"colab\": {\n",
    "   \"provenance\": [],\n",
    "   \"gpuType\": \"T4\"\n",
    "  },\n",
    "  \"accelerator\": \"GPU\"\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "7fb889c9e7a8fdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
