{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "!pip install sentence-transformers PyPDF2\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from google.colab import files\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# --- Colab bootstrap: clone repo if running on Colab ---\n",
    "import os, sys, subprocess\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB and not os.path.isdir('/content/noesisnoema-pipeline'):\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/raskolnikoff/noesisnoema-pipeline.git', '/content/noesisnoema-pipeline'], check=True)\n",
    "    sys.path.append('/content/noesisnoema-pipeline')\n",
    "\n",
    "# --- Model selection UI ---\n",
    "model_options = [\n",
    "    ('English MiniLM', 'all-MiniLM-L6-v2'),\n",
    "    ('Japanese/Multilingual (LaBSE)', 'sentence-transformers/LaBSE'),\n",
    "    ('Japanese S-BERT', 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'),\n",
    "]\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value='all-MiniLM-L6-v2',\n",
    "    description='Model:'\n",
    ")\n",
    "\n",
    "chunk_size_slider = widgets.IntSlider(\n",
    "    value=512,\n",
    "    min=50,\n",
    "    max=2048,\n",
    "    step=50,\n",
    "    description='Chunk size (tokens):'\n",
    ")\n",
    "\n",
    "overlap_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=0,\n",
    "    max=200,\n",
    "    step=10,\n",
    "    description='Overlap (tokens):'\n",
    ")\n",
    "\n",
    "display(model_dropdown)\n",
    "display(chunk_size_slider)\n",
    "display(overlap_slider)\n",
    "\n",
    "# PDF upload\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Initialize selected model\n",
    "model = SentenceTransformer(model_dropdown.value)\n",
    "\n",
    "# PDF to text extraction\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_reader = PyPDF2.PdfReader(open(pdf_path, \"rb\"))\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    # Check if extracted text is empty (image-only PDF)\n",
    "    if not text.strip():\n",
    "        print(\"No extractable text found: This PDF appears to be image-only. Skipping.\")\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# Use TokenChunker from repo (installed via Colab bootstrap or local path)\n",
    "import sys, os\n",
    "try:\n",
    "    from chunker import TokenChunker  # when running in the repo locally\n",
    "except Exception:\n",
    "    sys.path.append('/content/noesisnoema-pipeline')\n",
    "    from chunker import TokenChunker  # when running on Colab after clone\n",
    "\n",
    "def make_chunks_and_embeddings(text: str, chunk_size: int = 512, overlap: int = 50):\n",
    "    \"\"\"Create chunks and embeddings using the TokenChunker (token-based splitting).\"\"\"\n",
    "    chunker = TokenChunker(chunk_size=chunk_size, overlap=overlap)\n",
    "    chunks = chunker.chunk_text(text)\n",
    "    embeddings = model.encode(chunks)\n",
    "    return chunks, embeddings\n",
    "\n",
    "\n",
    "# New function: Save chunks.json, embeddings.npy, embeddings.csv, metadata.json together as a .zip archive\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "def export_zip_archive(base, chunks, embeddings, metadata):\n",
    "    \"\"\"\n",
    "    Save chunks, embeddings (both .npy and .csv formats), and metadata into a .zip archive.\n",
    "    chunks: list of text chunks\n",
    "    embeddings: numpy array\n",
    "    metadata: dict containing metadata information\n",
    "    \"\"\"\n",
    "    # Create files in memory buffer temporarily\n",
    "    chunks_json = json.dumps(chunks, ensure_ascii=False).encode('utf-8')\n",
    "    metadata_json = json.dumps(metadata, ensure_ascii=False).encode('utf-8')\n",
    "    embeddings_bytes_io = io.BytesIO()\n",
    "    np.save(embeddings_bytes_io, embeddings)\n",
    "    embeddings_bytes = embeddings_bytes_io.getvalue()\n",
    "    # Create CSV in-memory string buffer\n",
    "    embeddings_csv_io = io.StringIO()\n",
    "    np.savetxt(embeddings_csv_io, embeddings, delimiter=\",\")\n",
    "    embeddings_csv_str = embeddings_csv_io.getvalue().encode('utf-8')\n",
    "\n",
    "    zip_path = f\"/content/{base}.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.writestr(\"chunks.json\", chunks_json)\n",
    "        zf.writestr(\"embeddings.npy\", embeddings_bytes)\n",
    "        zf.writestr(\"embeddings.csv\", embeddings_csv_str)\n",
    "        zf.writestr(\"metadata.json\", metadata_json)\n",
    "        print(f\"Exported ZIP archive: {zip_path}\")\n",
    "        print(\"Included embeddings.csv in the ZIP archive as well.\")\n",
    "\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_manifest(embedder_name: str, chunk_size: int, overlap: int, offset_unit: str = 'char') -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"pack_version\": \"1.1\",\n",
    "        \"embedder\": {\n",
    "            \"name\": embedder_name,\n",
    "            \"version\": datetime.utcnow().strftime('%Y-%m-%d'),\n",
    "            \"sha256\": \"\",  # optional: provide model file hash if available\n",
    "            \"dim\": int(embeddings.shape[1]) if 'embeddings' in globals() else None,\n",
    "            \"normalize\": \"L2\",\n",
    "            \"text_norm\": [\"nfkc\", \"lower\"],\n",
    "        },\n",
    "        \"chunking\": {\n",
    "            \"strategy\": \"sentence+fixed\",\n",
    "            \"size\": int(chunk_size),\n",
    "            \"overlap\": int(overlap),\n",
    "            \"sentence_boundary\": True,\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"paragraph_offsets\": True,\n",
    "            \"offset_unit\": offset_unit,\n",
    "            \"bm25_stats\": True,\n",
    "            \"source_diversity_key\": \"doc_id\",\n",
    "            \"doc_timestamp\": True,\n",
    "        },\n",
    "        \"build\": {\n",
    "            \"built_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"hostname\": os.uname().nodename if hasattr(os, 'uname') else \"colab\",\n",
    "            \"pipeline_rev\": \"colab-notebook\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def write_citations_jsonl(path: str, doc_id: str, chunks: List[str]):\n",
    "    \"\"\"Create a minimal citations.jsonl with paragraph indices as offsets (char-based).\"\"\"\n",
    "    with open(path, 'a', encoding='utf-8') as f:\n",
    "        cursor = 0\n",
    "        for idx, ch in enumerate(chunks):\n",
    "            start = cursor\n",
    "            end = cursor + len(ch)\n",
    "            obj = {\n",
    "                \"chunk_id\": f\"{doc_id}#{idx:05d}\",\n",
    "                \"doc_id\": doc_id,\n",
    "                \"para_id\": f\"p-{idx:04d}\",\n",
    "                \"start\": int(start),\n",
    "                \"end\": int(end),\n",
    "                \"snippet\": ch[:200]\n",
    "            }\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            cursor = end\n",
    "\n",
    "for pdf_path in uploaded.keys():\n",
    "    print(f\"Processing {pdf_path} ...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    chunks, embeddings = make_chunks_and_embeddings(\n",
    "        text,\n",
    "        chunk_size=chunk_size_slider.value,\n",
    "        overlap=overlap_slider.value,\n",
    "    )\n",
    "\n",
    "    base = pdf_path.rsplit('.', 1)[0]\n",
    "    out_dir = f\"/content/{base}_pack\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save core artifacts\n",
    "    with open(f\"{out_dir}/chunks.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False)\n",
    "    np.save(f\"{out_dir}/embeddings.npy\", embeddings)\n",
    "    np.savetxt(f\"{out_dir}/embeddings.csv\", embeddings, delimiter=\",\")\n",
    "    metadata = {\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"doc_id\": base,\n",
    "                \"title\": base,\n",
    "                \"path\": f\"/content/{pdf_path}\",\n",
    "                \"page\": None,\n",
    "                \"line\": None,\n",
    "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    with open(f\"{out_dir}/metadata.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False)\n",
    "\n",
    "    # v1.1 extras\n",
    "    manifest = create_manifest(model_dropdown.value, chunk_size_slider.value, overlap_slider.value, offset_unit='char')\n",
    "    with open(f\"{out_dir}/pack.manifest.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False)\n",
    "\n",
    "    citations_path = f\"{out_dir}/citations.jsonl\"\n",
    "    # truncate if exists\n",
    "    open(citations_path, 'w', encoding='utf-8').close()\n",
    "    write_citations_jsonl(citations_path, base, chunks)\n",
    "\n",
    "    print(f\"Saved v1.1 pack to: {out_dir}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Quick validator (not a full schema check) ---\n",
    "import glob\n",
    "\n",
    "def quick_validate(pack_dir: str):\n",
    "    required = [\n",
    "        'pack.manifest.json',\n",
    "        'chunks.json',\n",
    "        'embeddings.npy',\n",
    "        'embeddings.csv',\n",
    "        'metadata.json',\n",
    "        'citations.jsonl',\n",
    "    ]\n",
    "    missing = [p for p in required if not os.path.exists(os.path.join(pack_dir, p))]\n",
    "    if missing:\n",
    "        print('Missing:', missing)\n",
    "        raise SystemExit(1)\n",
    "    with open(os.path.join(pack_dir, 'pack.manifest.json'), 'r', encoding='utf-8') as f:\n",
    "        man = json.load(f)\n",
    "    print('Manifest pack_version:', man.get('pack_version'))\n",
    "    print('Embedder:', man.get('embedder', {}).get('name'))\n",
    "    print('Chunks:', len(json.load(open(os.path.join(pack_dir, 'chunks.json'), 'r'))))\n",
    "    print('Embeddings shape:', np.load(os.path.join(pack_dir, 'embeddings.npy')).shape)\n",
    "    print('OK')\n",
    "\n",
    "# Example (validate the last built pack directory shown above)\n",
    "# quick_validate('/content/<your_base>_pack')\n",
    "export_zip_archive(base, chunks, embeddings, metadata)"
   ],
   "id": "2c2fb70d6f330e4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
